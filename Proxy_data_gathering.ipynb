{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable  \n",
    "import matplotlib\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "import sys\n",
    "sys.path.insert(0, '/srv/ccrc/data06/z5145948/Python/python_from_R/Holocene/sampled_models/plotting_files/')\n",
    "sys.path.insert(0, '/srv/ccrc/data06/z5145948/Deglaciation/modules/')\n",
    "from keyname import keyname as kn\n",
    "import config\n",
    "from plott import plott\n",
    "import scipy.interpolate\n",
    "# from mpl_toolkits.basemap import Basemap\n",
    "matplotlib.use('agg')\n",
    "from Cross_section import Cross_section\n",
    "from Proxy_graph_masked import Proxy_graph\n",
    "import Config\n",
    "# from Map_plot import Map_plot\n",
    "from collections import Counter\n",
    "import scipy.stats as stats\n",
    "from pylab import *\n",
    "rcParams['legend.numpoints'] = 1\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from IPython import embed\n",
    "import os\n",
    "import ast\n",
    "from matplotlib import gridspec\n",
    "import itertools\n",
    "import xarray as xr\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Set Formatting variables\n",
    "alpha = 0.5\n",
    "point_color_dp = (0,0,1,alpha)#'blue'\n",
    "point_color_shw = (1,0,0,alpha)#'red'\n",
    "line_color_dp = 'cyan'\n",
    "line_color_shw = 'magenta'\n",
    "lw = 4\n",
    "edgewidth=0.5\n",
    "size = 40\n",
    "fontsize = 20\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : fontsize}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.preamble'] = [\n",
    "    r'\\usepackage{wasysym}',\n",
    "    r'\\usepackage{textcomp}']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuctions for loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####3 reading data function\n",
    "\n",
    "def read_data(folder):\n",
    "\n",
    "    # Import simulation details (summary) as dataframe\n",
    "    summary = pd.read_csv(folder + '_summary.txt', delimiter = ' ')\n",
    "\n",
    "    # Import simulation outputs\n",
    "    fh = Dataset(folder + 'output.nc')\n",
    "    proxy_simulations = fh.variables['var1_1'][:] \n",
    "\n",
    "    # Import samples (proxy data)\n",
    "    samples = pd.read_csv( folder + '_samples.txt', delimiter = ' ')\n",
    "\n",
    "    # Drop all unnecessary rows in summary\n",
    "    summary = summary.drop(['type', 'Row.names', 'reps', 'model', 'dataset', 'filename', 'success.rate'], axis = 1)\n",
    "\n",
    "    # Convert run.no to netcdf file index\n",
    "    summary['run.no'] = summary['run.no'] - 1\n",
    "    samples['run.no'] = samples['run.no'] - 1\n",
    "    \n",
    "    return(proxy_simulations, samples, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for reading and including cores that are in the Oliver compilation but not in the Peterson data set\n",
    "\n",
    "def Oliver_cores(minn, maxx):\n",
    "    \n",
    "    # Obtain the relevant data\n",
    "    folder_location = '/srv/ccrc/data06/z5145948/Moving_water_mass/Data/Core_files/'\n",
    "    file_mat = ['GeoB4403_2.txt',\n",
    "             'GeoB1028_5.txt',\n",
    "             'GeoB2109_1.txt',\n",
    "             'GeoB3801_6.txt',\n",
    "             'V22_38.txt',\n",
    "             'V28_56.txt',\n",
    "             'V27_20.txt',\n",
    "             'RC12_339.txt',\n",
    "             'V32_128.txt',\n",
    "             'GIK16772_1.txt',\n",
    "             'MD96_2080.txt',\n",
    "             'MD06_3018.txt',\n",
    "             'NEAP18K.txt',\n",
    "             'KNR140_37JPC.txt']\n",
    "\n",
    "    # Specify which basin each core pertains to\n",
    "    location = ['Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Atlantic,','Indian,','Pacific,','Atlantic,','Atlantic,','Pacific,','Atlantic,','Atlantic,']\n",
    "\n",
    "    # Create an empty list to add the results to\n",
    "    oliver_data = []\n",
    "\n",
    "    # Loop over all the cores, read in the data and add to the list\n",
    "    i = 0\n",
    "    while i < len(file_mat):\n",
    "        with open(folder_location + file_mat[i]) as f:\n",
    "            for line in f:\n",
    "                oliver_data.append(location[i] + line)\n",
    "        i += 1\n",
    "\n",
    "    # Transform list into a dataframe\n",
    "    df_oliver = pd.DataFrame([sub.split(\",\") for sub in oliver_data])\n",
    "\n",
    "    # Rename the columns\n",
    "    df_oliver.columns = ['Location','Core','Lat','Lon','Dep','Core depth','age','Species','pl1','pl2','d18O benthic','d13C']\n",
    "\n",
    "    # get only the cores that are relevant\n",
    "    df_oliver = df_oliver[['Core','Location','Lat','Lon','Dep','d13C','age']]\n",
    "\n",
    "    # strip off any white spaces from the d13C results\n",
    "    df_oliver['d13C'] = [i.rstrip() for i in df_oliver['d13C']]\n",
    "\n",
    "    # get the data that falls within this period period\n",
    "    df_oliver = df_oliver[df_oliver['age'].astype(float) > float(minn)]\n",
    "    df_oliver = df_oliver[df_oliver['age'].astype(float) < float(maxx)]\n",
    "\n",
    "    ################################# other data                                                                                                                                                                                                                                  \n",
    "\n",
    "    # Obtain the relevant data\n",
    "    file_mat = ['CH69_K09.txt',\n",
    "    'MD03_2664.txt',\n",
    "    'MD95_2042.txt',\n",
    "    'U1308.txt',\n",
    "    'ODP1063.txt']\n",
    "    \n",
    "    # Specify all the auxiliary information about the cores\n",
    "    locations = ['CH69_K09\\tAtlantic\\t41.75\\t-47.35\\t4100\\t',\n",
    "              'MD03_2664\\tAtlantic\\t57.439000\\t-48.605800\\t3442.0\\t',\n",
    "              'MD95_2042\\tAtlantic\\t37.799833\\t-10.166500\\t3146.0\\t',\n",
    "              'U1308\\tAtlantic\\t49.877760\\t-24.238110\\t3871.0\\t',\n",
    "              'ODP1063\\tAtlantic\\t33.683333\\t-57.616667\\t4584\\t']\n",
    "\n",
    "    # Create an empty list to add the results to\n",
    "    other_data = []\n",
    "    i = 0\n",
    "\n",
    "    # Loop over all the files and append the results to the list\n",
    "    while i < len(file_mat):\n",
    "        with open(folder_location + file_mat[i]) as f:\n",
    "            for line in f:\n",
    "                other_data.append(locations[i]+line)\n",
    "        i += 1\n",
    "\n",
    "    # Create a dataframe of the other core data\n",
    "    df_other = pd.DataFrame([sub.split(\"\\t\") for sub in other_data])\n",
    "\n",
    "    # rename the columns\n",
    "    df_other.columns = ['Core','Location','Lat','Lon','Dep','Core Depth','age','d13C']\n",
    "\n",
    "    # get only the columns that we care about\n",
    "    df_other = df_other[['Core','Location','Lat','Lon','Dep','d13C','age']]\n",
    "\n",
    "    # strip any white spaces off of the d13C variable\n",
    "    df_other['d13C'] = [i.rstrip() for i in df_other['d13C']]\n",
    "\n",
    "    # Extract the results which fall within this period period\n",
    "    df_other = df_other[df_other['age'].astype(float) > float(minn)]\n",
    "    df_other = df_other[df_other['age'].astype(float) < float(maxx)]\n",
    "\n",
    "    # Combine the Oliver and other dataframes together\n",
    "    df_all_results = pd.concat([df_oliver, df_other])\n",
    "\n",
    "    # Drop any d13C results which are empty\n",
    "    df_all_results = df_all_results[df_all_results['d13C'] != '']\n",
    "\n",
    "    # Make sure all variables are in the right data type\n",
    "    df_all_results['d13C'] = df_all_results['d13C'].astype(float)\n",
    "    df_all_results['Lat'] = df_all_results['Lat'].astype(float)\n",
    "    df_all_results['Lon'] = df_all_results['Lon'].astype(float)\n",
    "    df_all_results['Dep'] = df_all_results['Dep'].astype(float)\n",
    "    \n",
    "    return(df_all_results)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def pl_cores(minn, maxx):\n",
    "\n",
    "    # Set the names of the pandas columns\n",
    "    names = ['Core', 'Location', 'Lat', 'Lon', 'Dep']\n",
    "\n",
    "    # Read in the data (three files; two for different basins and an addition file)\n",
    "    indopac = pd.read_table(\"../Moving_water_mass/Data/Core_files/indopac_core_data_LS16.txt\", delimiter = ',', names = names)\n",
    "    atl = pd.read_table(\"../Moving_water_mass/Data/Core_files/atl_core_data_LS16.txt\", delimiter = ',', names = names)\n",
    "    add = pd.read_table(\"../Moving_water_mass/Data/Core_files/Additional_core_locations.txt\", delimiter = ',', usecols = [0, 1, 2, 3, 4], names = names)\n",
    "\n",
    "    # Join all into a single dataframe\n",
    "    df_loc = indopac.append(atl)\n",
    "    df_loc = df_loc.append(add)\n",
    "    df_loc = df_loc.reset_index(drop = True)\n",
    "\n",
    "    # Determine the names of the files as they would be saved from their core names\n",
    "    df_loc['d18O names'] = df_loc['Core'] + '_ageLS16.txt'\n",
    "    df_loc['d13C names'] = df_loc['Core'] + '_d13C.txt'\n",
    "\n",
    "\n",
    "    # Create an empty dictionary for all the results, and row counter\n",
    "    i = 0\n",
    "    results_dict = {}\n",
    "\n",
    "    # Loop over the dataset and interpolate each core age model to d13C data    \n",
    "    while i < df_loc.count()[0]:\n",
    "\n",
    "        # Try getting the age model (d18O) and d13C data for a core. If you can't, move on to next core\n",
    "        try:\n",
    "            df_d18O = pd.read_table('../Moving_water_mass/Data/Core_files/' + df_loc.loc[i]['d18O names'], delim_whitespace = True, names = ['depth', 'age'], skip_blank_lines = True, na_values = 'NAN')\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        try:\n",
    "            df_d13C = pd.read_table('../Moving_water_mass/Data/Core_files/' + df_loc.loc[i]['d13C names'], delim_whitespace = True, names = ['depth', 'd13C'], skip_blank_lines = True, na_values = 'NAN')\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Preprocess data to remove NaNs\n",
    "        df_d18O = df_d18O.dropna(subset = ['age']) \n",
    "        df_d13C = df_d13C.dropna(subset = ['d13C'])\n",
    "\n",
    "        # Drop the meaningless index\n",
    "        df_d18O = df_d18O.reset_index(drop = True)\n",
    "        df_d13C = df_d13C.reset_index(drop = True)\n",
    "\n",
    "        # from the age model files, create an interpolation function\n",
    "        interp = scipy.interpolate.interp1d(df_d18O['depth'], df_d18O['age'], bounds_error = True)\n",
    "        try:\n",
    "            df_d13C['age'] = interp(df_d13C['depth'])\n",
    "        except:\n",
    "            try:\n",
    "                interp2 = scipy.interpolate.interp1d(df_d18O['depth'], df_d18O['age'], bounds_error = False)\n",
    "                df_d13C['age'] = interp2(df_d13C['depth'])\n",
    "            except:\n",
    "                i += 1\n",
    "                continue    \n",
    "    \n",
    "        # Drop ages where it's nan\n",
    "        df_d13C = df_d13C.dropna(subset = ['age'])\n",
    "        \n",
    "        # Drop the index. It's meaningless\n",
    "        df_d13C = df_d13C.reset_index(drop = True)\n",
    "\n",
    "        # get only the cores that are within this period period\n",
    "        df_d13C = df_d13C[df_d13C['age'] > minn]\n",
    "        df_d13C = df_d13C[df_d13C['age'] < maxx]\n",
    "\n",
    "        # If the dataframe is not empty\n",
    "        if len(df_d13C) > 0:\n",
    "            \n",
    "            # Drop irrelevant columns from the location dataframe\n",
    "            df_single_core_loc = df_loc.drop(['d18O names', 'd13C names'], axis = 1)\n",
    "            \n",
    "            # replicate a single core location data for the same number of columns as there are d13C rows\n",
    "            df_single_core_loc = df_single_core_loc.loc[\n",
    "                df_single_core_loc.index.repeat(len(df_d13C))].loc[[i]]\n",
    "\n",
    "            # Drop depth (not relevant)\n",
    "            df_d13C = df_d13C.drop(['depth'], axis = 1)\n",
    "\n",
    "            # Join the d13C data to the single core location data\n",
    "            df_single_core_loc = \\\n",
    "                df_single_core_loc.reset_index(drop = True).join(df_d13C.reset_index(drop = True))\n",
    "            \n",
    "            # Add the results of this core to the results dictionary\n",
    "            results_dict.update({\n",
    "                df_single_core_loc.Core[0] : df_single_core_loc.drop(['Core'], axis = 1)\n",
    "            })\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    # Join all the dataframes in the results dictionary into a single dataframe\n",
    "    df_results = pd.concat(results_dict).reset_index()\n",
    "    \n",
    "    # relabel core column\n",
    "    df_results = df_results.rename(columns = {'level_0' : 'Core'})\n",
    "    df_results = df_results.drop(['level_1'], axis = 1)\n",
    "\n",
    "    return(df_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slicing_data(period_min,period_max,location_filter):\n",
    "\n",
    "    # run the function to collect all of the relevant Peterson and Lisiecki cores\n",
    "    df_pl = pl_cores(period_min,period_max)\n",
    "\n",
    "    # run the function to collect all of the relevant Oliver cores\n",
    "    df_oliver = Oliver_cores(period_min,period_max)\n",
    "    \n",
    "    # Combine both sets of cores together\n",
    "    df_results = df_pl.append(df_oliver)\n",
    "\n",
    "    # Drop the index (it has no meaning)\n",
    "    df = df_results.reset_index(drop = True)\n",
    "    \n",
    "    # Make sure that all depths are positive (some provide their data with negative cores)\n",
    "    df['Dep'] = abs(df['Dep'])\n",
    "\n",
    "    # Get only the cores for this given ocean\n",
    "    df = df[df['Location'] == location_filter]\n",
    "    \n",
    "    # drop the index, since it has no meaning\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    # make sure that the core ages are given as floats (not strings)\n",
    "    df['age'] = df.age.astype(float)\n",
    "\n",
    "    # Slice the data to make a lower and upper period frame for the moving average\n",
    "    lower = np.arange(period_min, period_max, config.slice_moving_amount)\n",
    "    upper = np.arange(period_min+config.slice_width, period_max+config.slice_width, config.slice_moving_amount)\n",
    "\n",
    "    # Creat an empty dictionary \n",
    "    proxy_compilation = {}\n",
    "\n",
    "    # Loop through each of the period windows\n",
    "    for low, up in zip(lower, upper):\n",
    "        \n",
    "        # Get only the points that fall within this particular subslice\n",
    "        df_slice = df[(df['age'] >= low) & (df['age'] < up)]\n",
    "        \n",
    "        # group the slice based on cores\n",
    "        grouped_slice = df_slice.groupby(['Core'])\n",
    "        \n",
    "        # get the mean and count per core in this slice\n",
    "        df_slice_averaged = \\\n",
    "            grouped_slice.mean().join(grouped_slice.count()[['Lat']].rename(columns={'Lat':'count'}))\n",
    "        \n",
    "        # Rename the age column age_mean, since it is the average of the ages in that slice\n",
    "        df_slice_averaged.rename(columns={'age':'age_mean'},inplace=True)\n",
    "        \n",
    "        # get the average age of the slice, and add averaged dataframe to a dict under that index\n",
    "        proxy_compilation.update({\n",
    "            (low+up)/2 : df_slice_averaged\n",
    "        })\n",
    "        \n",
    "    # Transform dictionary of dataframes into a single dataframe\n",
    "    proxy_compilation = pd.concat(proxy_compilation,axis=0)\n",
    "    \n",
    "    # Reset the index, and name the age index column\n",
    "    proxy_compilation.reset_index(drop=False,inplace=True)\n",
    "    proxy_compilation.rename(columns={'level_0':'age'},inplace=True)\n",
    "    \n",
    "    return(df, proxy_compilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_interpolation(proxy_compilation):\n",
    "    # interpolating across the entire dataset\n",
    "\n",
    "    # Add time bounds to the samples table\n",
    "    samples_with_time_period = proxy_compilation\n",
    "\n",
    "    unique_cores = np.unique(samples_with_time_period['Core'])\n",
    "    years_list = np.unique(samples_with_time_period.age)\n",
    "\n",
    "    samples_with_time_period = samples_with_time_period.reset_index(drop=True)\n",
    "\n",
    "    interpolated_proxies = {}\n",
    "\n",
    "    for unique_core in unique_cores:\n",
    "\n",
    "        # get a single proxy\n",
    "        df_temp = samples_with_time_period[samples_with_time_period['Core'] == unique_core]\n",
    "\n",
    "        try:\n",
    "            # interpolate the dataset\n",
    "            interp = scipy.interpolate.interp1d(df_temp['age'],\n",
    "                                                df_temp['d13C'],\n",
    "                                                bounds_error = False)\n",
    "            single_proxy_interpolated = pd.DataFrame({'age' : years_list, 'd13C' : interp(years_list)})        \n",
    "            location_repeated = pd.concat([df_temp.reset_index(0).loc[0,['Lat', 'Lon', 'Dep','count']]] * len(single_proxy_interpolated), axis=1).T\n",
    "            single_proxy_interpolated = pd.concat([location_repeated.reset_index(drop=True), single_proxy_interpolated.reset_index(drop=True)],axis=1)    \n",
    "\n",
    "            interpolated_proxies.update({\n",
    "                unique_core : single_proxy_interpolated\n",
    "            })        \n",
    "\n",
    "        except ValueError:\n",
    "            interpolated_proxies.update({\n",
    "                unique_core : df_temp.drop('Core',axis=1)\n",
    "            })\n",
    "\n",
    "    interpolated_samples = pd.concat(interpolated_proxies).reset_index(drop=False).rename(columns={'level_0':'Core'}).drop(['level_1'],axis=1)\n",
    "    interpolated_samples_backup = interpolated_samples.copy()\n",
    "\n",
    "    # Drop count because this count doesn't make sense\n",
    "    interpolated_samples = interpolated_samples.drop('count',axis=1)\n",
    "\n",
    "    # Merge the original dataframe to get counts back (nan for interpolated samples)\n",
    "    interpolated_samples = pd.merge(interpolated_samples, proxy_compilation[['age','Core','count']],\n",
    "                                    how='outer', left_on=['age','Core'], right_on=['age','Core'])\n",
    "\n",
    "    # Drop all nan d13C values (interpolation tried but out of range)\n",
    "    interpolated_samples = interpolated_samples[np.isfinite(interpolated_samples['d13C'])]\n",
    "    \n",
    "    return(interpolated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latex_table(proxy_compilation,latex_name):\n",
    "    # Save list of cores to latex table to include in paper\n",
    "    latex_table = proxy_compilation.drop(['d13C','count','age'],axis=1)\n",
    "    latex_table['Reference'] = '-'\n",
    "    latex_table = latex_table.drop_duplicates()\n",
    "\n",
    "    # Rename columns\n",
    "    latex_table = latex_table.rename(columns={'Lat':'Latitude','Lon' : 'Longitude', 'Dep' : 'Depth (m)'})\n",
    "    latex_table['Latitude'] = [str(round(x, 2)) for x in latex_table.Latitude]\n",
    "    latex_table['Longitude'] = [str(round(x, 2)) for x in latex_table.Longitude]\n",
    "    latex_table.sort_values(by='Core',inplace=True)\n",
    "\n",
    "    # Convert to string of latex markdown\n",
    "    latex_string = latex_table.to_latex(index=False,longtable=True)\n",
    "\n",
    "    # Reformat some parts of the latex table\n",
    "    latex_string = latex_string.replace('\\\\toprule','')\n",
    "    latex_string = latex_string.replace('\\\\midrule','')\n",
    "    latex_string = latex_string.replace('\\\\bottomrule','')\n",
    "\n",
    "    # Write to a file\n",
    "    file1 = open(latex_name,\"w\") \n",
    "    file1.write(latex_string) \n",
    "    file1.close() #to change file access modes \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Atlantic_regions(proxy_compilation):\n",
    "    # Get volume weight average with period slices\n",
    "    proxy_compilation['weights'] = np.nan\n",
    "    proxy_compilation['regions'] = np.nan    \n",
    "\n",
    "    # NEA\n",
    "    proxy_compilation.loc[((proxy_compilation['Lon'] < 20) | (proxy_compilation['Lon'] > (-33))) & (proxy_compilation['Lat'] > 0.1),\n",
    "                            'weights'] = 4.3\n",
    "    proxy_compilation.loc[((proxy_compilation['Lon'] < 20) | (proxy_compilation['Lon'] > (-33))) & (proxy_compilation['Lat'] > 0.1),\n",
    "                            'regions'] = 'NEA'\n",
    "    \n",
    "\n",
    "    # NWA\n",
    "    proxy_compilation.loc[(proxy_compilation['Lon'] < (-33)) & (proxy_compilation['Lon'] > (-180)) & (proxy_compilation['Lat'] > 0.1),\n",
    "                             'weights'] = 4.9\n",
    "    proxy_compilation.loc[(proxy_compilation['Lon'] < (-33)) & (proxy_compilation['Lon'] > (-180)) & (proxy_compilation['Lat'] > 0.1),\n",
    "                             'regions'] = 'NWA'\n",
    "\n",
    "    # SEA\n",
    "    proxy_compilation.loc[((proxy_compilation['Lon'] < 30) | (proxy_compilation['Lon'] > (-14.6))) & (proxy_compilation['Lat'] < 0) & (proxy_compilation['Lat'] > -55),\n",
    "                             'weights'] = 3.5\n",
    "    proxy_compilation.loc[((proxy_compilation['Lon'] < 30) | (proxy_compilation['Lon'] > (-14.6))) & (proxy_compilation['Lat'] < 0) & (proxy_compilation['Lat'] > -55),\n",
    "                             'regions'] = 'SEA'\n",
    "    \n",
    "    # SA\n",
    "    proxy_compilation.loc[((proxy_compilation['Lon'] < 30) | (proxy_compilation['Lon'] > (-22))) & (proxy_compilation['Lat'] < -40) & (proxy_compilation['Lat'] > -55),\n",
    "                            'weights'] = 0.7\n",
    "    proxy_compilation.loc[((proxy_compilation['Lon'] < 30) | (proxy_compilation['Lon'] > (-22))) & (proxy_compilation['Lat'] < -40) & (proxy_compilation['Lat'] > -55),\n",
    "                            'regions'] = 'SA'    \n",
    "\n",
    "    # SWA\n",
    "    proxy_compilation.loc[(proxy_compilation['Lon'] > (-60)) & (proxy_compilation['Lon'] < (-14.6)) & (proxy_compilation['Lat'] < 0) & (proxy_compilation['Lat'] > -55),\n",
    "                            'weights'] = 5.0\n",
    "    proxy_compilation.loc[(proxy_compilation['Lon'] > (-60)) & (proxy_compilation['Lon'] < (-14.6)) & (proxy_compilation['Lat'] < 0) & (proxy_compilation['Lat'] > -55),\n",
    "                            'regions'] = 'SWA'    \n",
    "    \n",
    "    return(proxy_compilation)\n",
    "    \n",
    "def Pacific_regions(proxy_compilation):\n",
    "    # Get volume weight average with period slices\n",
    "    proxy_compilation['weights'] = np.nan\n",
    "    proxy_compilation['regions'] = np.nan      \n",
    "\n",
    "    # North\n",
    "    proxy_compilation.loc[(proxy_compilation['Lat'] > 0) ,'weights'] = 21.2\n",
    "    proxy_compilation.loc[(proxy_compilation['Lat'] > 0) ,'regions'] = 'NP'    \n",
    "\n",
    "    # South\n",
    "    proxy_compilation.loc[(proxy_compilation['Lat'] < 0) ,'weights'] = 23.9\n",
    "    proxy_compilation.loc[(proxy_compilation['Lat'] < 0) ,'regions'] = 'SP'\n",
    "    \n",
    "    return(proxy_compilation)\n",
    "\n",
    "def Indian_regions(proxy_compilation):\n",
    "    # Get volume weight average with period slices # Use a single region for the Indian Ocean\n",
    "    proxy_compilation['weights'] = 1\n",
    "    proxy_compilation['regions'] = 'I'\n",
    "    \n",
    "    return(proxy_compilation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaging_by_region(proxy_compilation):\n",
    "    \n",
    "    # group the cores based on the age and the region (indicated by weights)\n",
    "    grouped_by_age_region = proxy_compilation.groupby(['age', 'regions'])\n",
    "    \n",
    "    averaged_by_age_region = {}\n",
    "    stdev_by_age_region = {}\n",
    "    measurement_count = {}\n",
    "    core_count = {}\n",
    "    CI_by_age_region = {}\n",
    "    \n",
    "    for key, group in grouped_by_age_region:\n",
    "        \n",
    "        # find group means\n",
    "        averaged_by_age_region.update({\n",
    "            key: np.mean(group)\n",
    "        })\n",
    "        # find total number of cores\n",
    "        core_count.update({\n",
    "            key: len(group['count'].dropna())\n",
    "        }) \n",
    "        # find total measurement count\n",
    "        measurement_count.update({\n",
    "            key: np.nansum(group['count'])\n",
    "        })\n",
    "        # find the standard deviation across the slices and cores\n",
    "        stdev_by_age_region.update({\n",
    "            key: np.std(group['d13C'])\n",
    "        })\n",
    "        # express variation as a confidence interval\n",
    "        CI_by_age_region.update({\n",
    "            key: 1.96 * np.std(group['d13C'])/(np.sqrt(len(group['count'].dropna()))) #CI of 95%, 1.96 is zscore\n",
    "        })        \n",
    "        \n",
    "    # Convert dictionaries to dataframes\n",
    "    averaged_by_age_region = pd.concat(averaged_by_age_region,axis=1).T.reset_index(drop=False).rename({'level_1':'regions'},axis=1).drop('level_0',axis=1)\n",
    "    measurement_count = pd.DataFrame.from_dict(measurement_count,orient='index')\n",
    "    core_count = pd.DataFrame.from_dict(core_count,orient='index')\n",
    "    stdev_by_age_region = pd.DataFrame.from_dict(stdev_by_age_region,orient='index')\n",
    "    CI_by_age_region = pd.DataFrame.from_dict(CI_by_age_region,orient='index')    \n",
    "    \n",
    "    # Add columns of number of cores and number of measurements to the dataframe\n",
    "    averaged_by_age_region['measurement_count'] = list(measurement_count[0])\n",
    "    averaged_by_age_region['core_count'] = list(core_count[0])  \n",
    "    averaged_by_age_region['slice_stdev'] = list(stdev_by_age_region[0])\n",
    "    averaged_by_age_region['CI'] = list(CI_by_age_region[0])\n",
    "    \n",
    "    # Now there is one values for each region (weight) and each year combination\n",
    "    # Group by years and use weights to find the average d13C for that period period\n",
    "    grouped_by_age = averaged_by_age_region.drop('regions',axis=1).groupby('age')\n",
    "\n",
    "    # Drop the core details, since we've averaged by regions in this df, and we don't need them now\n",
    "    averaged_by_age_region = averaged_by_age_region.drop(['count','Lat','Lon','Dep'],axis=1)    \n",
    "    \n",
    "    # Make empty dictionaries to store results\n",
    "    averaged_by_age = {}\n",
    "    stdev_by_age = {}\n",
    "\n",
    "    # find group means by looping over age-region groups\n",
    "    for key, group in grouped_by_age:\n",
    "\n",
    "        # Find the volume weighted average of the age-region group\n",
    "        averaged_by_age.update({\n",
    "            np.mean(group['age']) : np.sum(group['d13C'] * group['weights'])/np.sum(group['weights'])\n",
    "        })\n",
    "        \n",
    "        # Find the normal average\n",
    "        avg = np.mean(group.d13C)\n",
    "        \n",
    "        # Use this average to find the standard deviation of the age-region group\n",
    "        stdev_by_age.update({\n",
    "            np.mean(group['age']) : np.average((group.d13C-avg)**2,weights=group.weights)\n",
    "        }) \n",
    "\n",
    "    # Convert results dicts to dataframes\n",
    "    averaged_by_age = pd.DataFrame.from_dict(averaged_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'age', 0 : 'd13C'})\n",
    "    stdev_by_age = pd.DataFrame.from_dict(stdev_by_age,orient='index').reset_index(drop=False).rename(columns={'index' : 'age', 0 : 'stdev'})\n",
    "\n",
    "    # Join the two results (avg and stdev) to a single df\n",
    "    averaged_by_age = pd.merge(left=averaged_by_age,right=stdev_by_age)\n",
    "    \n",
    "    # Order the results by age\n",
    "    averaged_by_age = averaged_by_age.sort_values('age')\n",
    "    \n",
    "    return(averaged_by_age,averaged_by_age_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results_dict,period,basin,proxy_compilation,averaged_by_age_region,averaged_by_age,df_all_data):\n",
    "\n",
    "    # Save raw d13C results (all the data)\n",
    "    results_dict.update({\n",
    "        kn(period=period[1],basin=basin,vartype='df_d13C_unsliced'): \\\n",
    "        df_all_data.to_dict()\n",
    "    })\n",
    "\n",
    "    # Save raw d13C results (all the data)\n",
    "    results_dict.update({\n",
    "        kn(period=period[1],basin=basin,vartype='df_d13C_raw'): \\\n",
    "        proxy_compilation.to_dict()\n",
    "    })\n",
    "        \n",
    "    # Save the d13C data which has been grouped into age and region\n",
    "    results_dict.update({\n",
    "        kn(period=period[1],basin=basin,vartype='d13C_avg_age_region'): \\\n",
    "        averaged_by_age_region.to_dict()\n",
    "    })\n",
    "        \n",
    "    # Save the d13C data that is just a time series, and the volume weightings have been considered\n",
    "    results_dict.update({\n",
    "        kn(period=period[1],basin=basin,vartype='d13C_avg_age'): \\\n",
    "        averaged_by_age.to_dict()\n",
    "    })       \n",
    "\n",
    "    return(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/ccrc/data06/z5145948/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/srv/ccrc/data06/z5145948/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Create an empty dictionary to store the results in\n",
    "results_dict = {}\n",
    "\n",
    "# List the different variables that can be combined\n",
    "basin = ['Atlantic','Pacific','Indian']\n",
    "\n",
    "# Find all possible combinations of period, location and depth\n",
    "combinations = pd.DataFrame(list(itertools.product(*[config.period_list,basin]))).rename(\n",
    "    columns={0:'period',1:'basin'})\n",
    "\n",
    "# Loop over all the combinations\n",
    "for period,basin in zip(\n",
    "    combinations['period'],\n",
    "    combinations['basin']\n",
    "    ):\n",
    "    \n",
    "    # Define the region function to used based on what basin it is\n",
    "    if basin=='Atlantic':\n",
    "        region_func = Atlantic_regions\n",
    "    elif basin=='Pacific':\n",
    "        region_func = Pacific_regions\n",
    "    elif basin=='Indian':\n",
    "        region_func = Indian_regions\n",
    "    \n",
    "    # Run function for slicing the data, getting all samples and averaged by period slice\n",
    "    df_all_data, proxy_compilation = slicing_data(np.min(period[0]),np.max(period[0]),basin)\n",
    "    \n",
    "    # Interpolate samples\n",
    "    interpolated_samples = slice_interpolation(proxy_compilation)\n",
    "    \n",
    "    # get region weights\n",
    "    interpolated_samples = region_func(interpolated_samples)\n",
    "    \n",
    "    # Get regional average and ocean basin averages\n",
    "    averaged_by_age, averaged_by_age_region = averaging_by_region(interpolated_samples)    \n",
    "    \n",
    "    # Save the results\n",
    "    save_results(results_dict,period,basin,interpolated_samples,averaged_by_age_region,averaged_by_age,df_all_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary to Json and clear vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/results_dict.json', 'w') as fp:\n",
    "    json.dump(results_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "599px",
    "left": "1267px",
    "right": "20px",
    "top": "110px",
    "width": "595px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
